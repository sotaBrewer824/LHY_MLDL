{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Homework 6 - Generative Adversarial Network\nThis is the sample code for hw6 of 2022 Machine Learning course in National Taiwan University. \n\nIn this sample code, there are 5 sections:\n1. Environment setting\n2. Dataset preparation\n3. Model setting\n4. Train\n5. Inference\n\nYour goal is to do anime face generation, if you have any question, please discuss at NTU COOL ","metadata":{"id":"Iv6bjjqyGmqV"}},{"cell_type":"markdown","source":"# Environment setting\nIn this section, we will prepare for the dataset and set some environment variable","metadata":{"id":"xnp-5lUFLak7"}},{"cell_type":"markdown","source":"## Download Dataset","metadata":{"id":"_qhoMUt9LniJ"}},{"cell_type":"code","source":"# get dataset from huggingface hub\n!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n!apt-get install git-lfs\n!git lfs install\n!git clone https://huggingface.co/datasets/LeoFeng/MLHW_6\n!unzip ./MLHW_6/faces.zip -d .","metadata":{"id":"AaJRTJEFLrND"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Other setting","metadata":{"id":"lBkkAB9sO3R4"}},{"cell_type":"code","source":"# import module\nimport os\nimport glob\nimport random\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport logging\nfrom tqdm import tqdm\n\n\n# seed setting\ndef same_seeds(seed):\n    # Python built-in random module\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Torch\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nsame_seeds(2022)\nworkspace_dir = '.'","metadata":{"id":"Qxf1TXTLO6Ek"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset preparation\nIn this section, we prepare for the dataset for Pytorch","metadata":{"id":"eg2qsevzOeQT"}},{"cell_type":"markdown","source":"## Create dataset for Pytorch\n\nIn order to unified image information, we use the transform function to:\n1. Resize image to 64x64\n2. Normalize the image\n\nThis CrypkoDataset class will be use in Section 4","metadata":{"id":"UT6s1x92OudB"}},{"cell_type":"code","source":"# prepare for CrypkoDataset\n\nclass CrypkoDataset(Dataset):\n    def __init__(self, fnames, transform):\n        self.transform = transform\n        self.fnames = fnames\n        self.num_samples = len(self.fnames)\n\n    def __getitem__(self,idx):\n        fname = self.fnames[idx]\n        img = torchvision.io.read_image(fname)\n        img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return self.num_samples\n\ndef get_dataset(root):\n    fnames = glob.glob(os.path.join(root, '*'))\n    compose = [\n        transforms.ToPILImage(),\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ]\n    transform = transforms.Compose(compose)\n    dataset = CrypkoDataset(fnames, transform)\n    return dataset","metadata":{"id":"9MsHqaglOywi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Show the image\nShow some sample in the dataset","metadata":{"id":"BPMZTwAiQSnx"}},{"cell_type":"code","source":"temp_dataset = get_dataset(os.path.join(workspace_dir, 'faces'))\n\nimages = [temp_dataset[i] for i in range(4)]\ngrid_img = torchvision.utils.make_grid(images, nrow=4)\nplt.figure(figsize=(10,10))\nplt.imshow(grid_img.permute(1, 2, 0))\nplt.show()","metadata":{"id":"rX5-Q71TOyy4","outputId":"5ccce544-982c-4635-c88a-3a0290985669"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model setting\nIn this section, we will create models and trainer.","metadata":{"id":"IgV-jpcfQwEM"}},{"cell_type":"markdown","source":"## Create model\nIn this section, we will create models for Generator and Discriminator","metadata":{"id":"EY4rAlw8RNhG"}},{"cell_type":"code","source":"# Generator\n\nclass Generator(nn.Module):\n    \"\"\"\n    Input shape: (batch, in_dim)\n    Output shape: (batch, 3, 64, 64)\n    \"\"\"\n    def __init__(self, in_dim, feature_dim=64):\n        super().__init__()\n    \n        #input: (batch, 100)\n        self.l1 = nn.Sequential(\n            nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False),\n            nn.BatchNorm1d(feature_dim * 8 * 4 * 4),\n            nn.ReLU()\n        )\n        self.l2 = nn.Sequential(\n            self.dconv_bn_relu(feature_dim * 8, feature_dim * 4),               #(batch, feature_dim * 16, 8, 8)     \n            self.dconv_bn_relu(feature_dim * 4, feature_dim * 2),               #(batch, feature_dim * 16, 16, 16)     \n            self.dconv_bn_relu(feature_dim * 2, feature_dim),                   #(batch, feature_dim * 16, 32, 32)     \n        )\n        self.l3 = nn.Sequential(\n            nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2,\n                               padding=2, output_padding=1, bias=False),\n            nn.Tanh()   \n        )\n        self.apply(weights_init)\n    def dconv_bn_relu(self, in_dim, out_dim):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2,\n                               padding=2, output_padding=1, bias=False),        #double height and width\n            nn.BatchNorm2d(out_dim),\n            nn.ReLU(True)\n        )\n    def forward(self, x):\n        y = self.l1(x)\n        y = y.view(y.size(0), -1, 4, 4)\n        y = self.l2(y)\n        y = self.l3(y)\n        return y","metadata":{"id":"8dfregFtRVGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discriminator\nclass Discriminator(nn.Module):\n    \"\"\"\n    Input shape: (batch, 3, 64, 64)\n    Output shape: (batch)\n    \"\"\"\n    def __init__(self, in_dim, feature_dim=64):\n        super(Discriminator, self).__init__()\n            \n        #input: (batch, 3, 64, 64)\n        \"\"\"\n        NOTE FOR SETTING DISCRIMINATOR:\n\n        Remove last sigmoid layer for WGAN\n        \"\"\"\n        self.l1 = nn.Sequential(\n            nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32)\n            nn.LeakyReLU(0.2),\n            self.conv_bn_lrelu(feature_dim, feature_dim * 2),                   #(batch, 3, 16, 16)\n            self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4),               #(batch, 3, 8, 8)\n            self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8),               #(batch, 3, 4, 4)\n            nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0),\n            #nn.Sigmoid() \n        )\n        self.apply(weights_init)\n    def conv_bn_lrelu(self, in_dim, out_dim):\n        \"\"\"\n        NOTE FOR SETTING DISCRIMINATOR:\n\n        You can't use nn.Batchnorm for WGAN-GP\n        Use nn.InstanceNorm2d instead\n        \"\"\"\n\n        return nn.Sequential(\n            nn.Conv2d(in_dim, out_dim, 4, 2, 1),\n            nn.BatchNorm2d(out_dim),\n            nn.LeakyReLU(0.2),\n        )\n    def forward(self, x):\n        y = self.l1(x)\n        y = y.view(-1)\n        return y","metadata":{"id":"IHykBwExRr0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting for weight init function\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","metadata":{"id":"Hb7Y38bsR35o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create trainer\nIn this section, we will create a trainer which contains following functions:\n1. prepare_environment: prepare the overall environment, construct the models, create directory for the log and ckpt\n2. train: train for generator and discriminator, you can try to modify the code here to construct WGAN or WGAN-GP\n3. inference: after training, you can pass the generator ckpt path into it and the function will save the result for you","metadata":{"id":"eC-6M2P3SAu9"}},{"cell_type":"code","source":"class TrainerGAN():\n    def __init__(self, config):\n        self.config = config\n        \n        self.G = Generator(100)\n        self.D = Discriminator(3)\n        \n        self.loss = nn.BCELoss()\n\n        \"\"\"\n        NOTE FOR SETTING OPTIMIZER:\n\n        GAN: use Adam optimizer\n        WGAN: use RMSprop optimizer\n        WGAN-GP: use Adam optimizer \n        \"\"\"\n        # self.opt_D = torch.optim.Adam(self.D.parameters(), lr=self.config[\"lr\"], betas=(0.5, 0.999))\n        # self.opt_G = torch.optim.Adam(self.G.parameters(), lr=self.config[\"lr\"], betas=(0.5, 0.999))\n        \n        self.opt_D = torch.optim.RMSprop(self.D.parameters(), lr=self.config[\"lr\"])\n        self.opt_G = torch.optim.RMSprop(self.G.parameters(), lr=self.config[\"lr\"])\n        \n        self.dataloader = None\n        self.log_dir = os.path.join(self.config[\"workspace_dir\"], 'logs')\n        self.ckpt_dir = os.path.join(self.config[\"workspace_dir\"], 'checkpoints')\n        \n        FORMAT = '%(asctime)s - %(levelname)s: %(message)s'\n        logging.basicConfig(level=logging.INFO, \n                            format=FORMAT,\n                            datefmt='%Y-%m-%d %H:%M')\n        \n        self.steps = 0\n        self.z_samples = Variable(torch.randn(100, self.config[\"z_dim\"])).cuda()\n        \n    def prepare_environment(self):\n        \"\"\"\n        Use this funciton to prepare function\n        \"\"\"\n        os.makedirs(self.log_dir, exist_ok=True)\n        os.makedirs(self.ckpt_dir, exist_ok=True)\n        \n        # update dir by time\n        time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        self.log_dir = os.path.join(self.log_dir, time+f'_{self.config[\"model_type\"]}')\n        self.ckpt_dir = os.path.join(self.ckpt_dir, time+f'_{self.config[\"model_type\"]}')\n        os.makedirs(self.log_dir)\n        os.makedirs(self.ckpt_dir)\n        \n        # create dataset by the above function\n        dataset = get_dataset(os.path.join(self.config[\"workspace_dir\"], 'faces'))\n        self.dataloader = DataLoader(dataset, batch_size=self.config[\"batch_size\"], shuffle=True, num_workers=2)\n        \n        # model preparation\n        self.G = self.G.cuda()\n        self.D = self.D.cuda()\n        self.G.train()\n        self.D.train()\n    def gp(self):\n        \"\"\"\n        Implement gradient penalty function\n        \"\"\"\n        pass\n        \n    def train(self):\n        \"\"\"\n        Use this function to train generator and discriminator\n        \"\"\"\n        self.prepare_environment()\n        \n        for e, epoch in enumerate(range(self.config[\"n_epoch\"])):\n            progress_bar = tqdm(self.dataloader)\n            progress_bar.set_description(f\"Epoch {e+1}\")\n            for i, data in enumerate(progress_bar):\n                imgs = data.cuda()\n                bs = imgs.size(0)\n\n                # *********************\n                # *    Train D        *\n                # *********************\n                z = Variable(torch.randn(bs, self.config[\"z_dim\"])).cuda()\n                r_imgs = Variable(imgs).cuda()\n                f_imgs = self.G(z)\n                # r_label = torch.ones((bs)).cuda()\n                # f_label = torch.zeros((bs)).cuda()\n\n\n                # Discriminator forwarding\n                r_logit = self.D(r_imgs)\n                f_logit = self.D(f_imgs)\n\n                \"\"\"\n                NOTE FOR SETTING DISCRIMINATOR LOSS:\n                \n                GAN: \n                    loss_D = (r_loss + f_loss)/2\n                WGAN: \n                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n                WGAN-GP: \n                    gradient_penalty = self.gp(r_imgs, f_imgs)\n                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit) + gradient_penalty\n                \"\"\"\n                # Loss for discriminator\n                # r_loss = self.loss(r_logit, r_label)\n                # f_loss = self.loss(f_logit, f_label)\n                # loss_D = (r_loss + f_loss) / 2\n                \n                loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n\n                # Discriminator backwarding\n                self.D.zero_grad()\n                loss_D.backward()\n                self.opt_D.step()\n\n                \"\"\"\n                NOTE FOR SETTING WEIGHT CLIP:\n                \n                WGAN: below code\n                \"\"\"\n                for p in self.D.parameters():\n                    p.data.clamp_(-self.config[\"clip_value\"], self.config[\"clip_value\"])\n\n\n\n                # *********************\n                # *    Train G        *\n                # *********************\n                if self.steps % self.config[\"n_critic\"] == 0:\n                    # Generate some fake images.\n                    z = Variable(torch.randn(bs, self.config[\"z_dim\"])).cuda()\n                    f_imgs = self.G(z)\n\n                    # Generator forwarding\n                    f_logit = self.D(f_imgs)\n\n\n                    \"\"\"\n                    NOTE FOR SETTING LOSS FOR GENERATOR:\n                    \n                    GAN: loss_G = self.loss(f_logit, r_label)\n                    WGAN: loss_G = -torch.mean(self.D(f_imgs))\n                    WGAN-GP: loss_G = -torch.mean(self.D(f_imgs))\n                    \"\"\"\n                    # Loss for the generator.\n                    # loss_G = self.loss(f_logit, r_label)\n                    \n                    loss_G = -torch.mean(self.D(f_imgs))\n\n                    # Generator backwarding\n                    self.G.zero_grad()\n                    loss_G.backward()\n                    self.opt_G.step()\n                    \n                if self.steps % 10 == 0:\n                    progress_bar.set_postfix(loss_G=loss_G.item(), loss_D=loss_D.item())\n                self.steps += 1\n\n            self.G.eval()\n            f_imgs_sample = (self.G(self.z_samples).data + 1) / 2.0\n            filename = os.path.join(self.log_dir, f'Epoch_{epoch+1:03d}.jpg')\n            torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n            logging.info(f'Save some samples to {filename}.')\n\n            # Show some images during training.\n            grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n            plt.figure(figsize=(10,10))\n            plt.imshow(grid_img.permute(1, 2, 0))\n            plt.show()\n\n            self.G.train()\n\n            if (e+1) % 5 == 0 or e == 0:\n                # Save the checkpoints.\n                torch.save(self.G.state_dict(), os.path.join(self.ckpt_dir, f'G_{e}.pth'))\n                torch.save(self.D.state_dict(), os.path.join(self.ckpt_dir, f'D_{e}.pth'))\n\n        logging.info('Finish training')\n\n    def inference(self, G_path, n_generate=1000, n_output=30, show=False):\n        \"\"\"\n        1. G_path is the path for Generator ckpt\n        2. You can use this function to generate final answer\n        \"\"\"\n\n        self.G.load_state_dict(torch.load(G_path))\n        self.G.cuda()\n        self.G.eval()\n        z = Variable(torch.randn(n_generate, self.config[\"z_dim\"])).cuda()\n        imgs = (self.G(z).data + 1) / 2.0\n        \n        os.makedirs('output', exist_ok=True)\n        for i in range(n_generate):\n            torchvision.utils.save_image(imgs[i], f'output/{i+1}.jpg')\n        \n        if show:\n            row, col = n_output//10 + 1, 10\n            grid_img = torchvision.utils.make_grid(imgs[:n_output].cpu(), nrow=row)\n            plt.figure(figsize=(row, col))\n            plt.imshow(grid_img.permute(1, 2, 0))\n            plt.show()","metadata":{"id":"f8ajFDWBTRzn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train\nIn this section, we will first set the config for trainer, then use it to train generator and discriminator","metadata":{"id":"-uf8BdVoYNJ8"}},{"cell_type":"markdown","source":"## Set config","metadata":{"id":"ykjfugCdYmYS"}},{"cell_type":"code","source":"config = {\n    \"model_type\": \"WGAN-WC\",\n    \"batch_size\": 64,\n    \"lr\": 1e-4,\n    \"n_epoch\": 50,\n    \"n_critic\": 2,\n    \"z_dim\": 100,\n    \"workspace_dir\": workspace_dir, # define in the environment setting\n    \"clip_value\":1\n}","metadata":{"id":"Jg4YdRVPYJSj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start to train","metadata":{"id":"ntn56Ffvip-x"}},{"cell_type":"code","source":"trainer = TrainerGAN(config)\ntrainer.train()","metadata":{"id":"NTHoXrLUYJUn","outputId":"ef81adc1-56f4-4181-e0d1-c1490d7e9266"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\nIn this section, we will use trainer to train model","metadata":{"id":"4g3_RUzYix0W"}},{"cell_type":"markdown","source":"## Inference through trainer","metadata":{"id":"T6hdMgj_i3kk"}},{"cell_type":"markdown","source":"# save the 1000 images into ./output folder\ntrainer.inference(f'{workspace_dir}/checkpoints/2022-03-31_15-59-17_GAN/G_0.pth') # you have to modify the path when running this line","metadata":{"id":"72EEf52FrOCp"}},{"cell_type":"markdown","source":"## Prepare .tar file for submission","metadata":{"id":"WuoaEVUgk7oZ"}},{"cell_type":"markdown","source":"%cd output\n!tar -zcf ../submission.tgz *.jpg\n%cd ..","metadata":{"id":"QI2cnbbWlA3Z","outputId":"7e2d3e13-250b-4820-b5bb-acaacb6398e4"}}]}